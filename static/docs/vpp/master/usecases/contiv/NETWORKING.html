

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Contiv/VPP Network Operation &mdash; The Vector Packet Processor 20.01 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/language_data.js"></script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/rules.css" type="text/css" />
    <link rel="author" title="About these documents" href="../../about.html" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Setting up a Node with a Single NIC" href="SINGLE_NIC_SETUP.html" />
    <link rel="prev" title="Preparing a VmWare Fusion Host" href="VMWARE_FUSION_HOST.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html" class="icon icon-home"> The Vector Packet Processor
          

          
            
            <img src="../../_static/fd-io_red_white.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                master
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../whatisvpp/index.html">The Vector Packet Processor (VPP)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../gettingstarted/index.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../links/index.html">VPP Wiki, Doxygen and Other Links</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Use Cases</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../containers.html">VPP with Containers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../simpleperf/index.html">VPP with Iperf3 and TRex</a></li>
<li class="toctree-l2"><a class="reference internal" href="../vhost/index.html">FD.io VPP with Virtual Machines</a></li>
<li class="toctree-l2"><a class="reference internal" href="../vmxnet3.html">VPP with VMware/Vmxnet3</a></li>
<li class="toctree-l2"><a class="reference internal" href="../acls.html">Access Control Lists (ACLs) with FD.io VPP</a></li>
<li class="toctree-l2"><a class="reference internal" href="../vppcloud.html">VPP inside the Cloud</a></li>
<li class="toctree-l2"><a class="reference internal" href="../homegateway.html">Using VPP as a Home Gateway</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="index.html">Contiv/VPP</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="K8s_Overview.html">Contiv/VPP Kubernetes Network Plugin</a></li>
<li class="toctree-l3"><a class="reference internal" href="SECURITY.html">Security</a></li>
<li class="toctree-l3"><a class="reference internal" href="Vagrant.html">Contiv-VPP Vagrant Installation</a></li>
<li class="toctree-l3"><a class="reference internal" href="MANUAL_INSTALL.html">Manual Installation</a></li>
<li class="toctree-l3"><a class="reference internal" href="VPP_CONFIG.html">Creating VPP Startup Configuration</a></li>
<li class="toctree-l3"><a class="reference internal" href="VMWARE_FUSION_HOST.html">Preparing a VmWare Fusion Host</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">Contiv/VPP Network Operation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#contiv-vpp-ipam-ip-address-management">Contiv/VPP IPAM (IP Address Management)</a></li>
<li class="toctree-l4"><a class="reference internal" href="#vpp-programming">VPP Programming</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="SINGLE_NIC_SETUP.html">Setting up a Node with a Single NIC</a></li>
<li class="toctree-l3"><a class="reference internal" href="MULTI_NIC_SETUP.html">Setting Up a Node with Multiple NICs</a></li>
<li class="toctree-l3"><a class="reference internal" href="CUSTOM_MGMT_NETWORK.html">Setting Up a Custom Management Network on Multi-Homed Nodes</a></li>
<li class="toctree-l3"><a class="reference internal" href="Prometheus.html">Prometheus Statistics</a></li>
<li class="toctree-l3"><a class="reference internal" href="VPP_PACKET_TRACING_K8S.html">How to do VPP Packet Tracing in Kubernetes</a></li>
<li class="toctree-l3"><a class="reference internal" href="VPPTRACE.html">Using vpptrace.sh for VPP Packet Tracing</a></li>
<li class="toctree-l3"><a class="reference internal" href="CORE_FILES.html">Capturing VPP core dumps</a></li>
<li class="toctree-l3"><a class="reference internal" href="BUG_REPORTS.html">Debugging and Reporting Bugs in Contiv-VPP</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../networksim.html">Network Simulator Plugin</a></li>
<li class="toctree-l2"><a class="reference internal" href="../webapp.html">Building VPP web applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="../container_test.html">Container-based network simulation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../container_test.html#os-distro-test-results">OS / Distro test results</a></li>
<li class="toctree-l2"><a class="reference internal" href="../container_test.html#proxy-server">Proxy Server</a></li>
<li class="toctree-l2"><a class="reference internal" href="../container_test.html#install-and-configure-lxd">Install and configure lxd</a></li>
<li class="toctree-l2"><a class="reference internal" href="../container_test.html#create-three-network-segments">Create three network segments</a></li>
<li class="toctree-l2"><a class="reference internal" href="../container_test.html#set-up-the-default-container-profile">Set up the default container profile</a></li>
<li class="toctree-l2"><a class="reference internal" href="../container_test.html#set-up-the-network-configurations">Set up the network configurations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../container_test.html#create-a-master-container-image">Create a “master” container image</a></li>
<li class="toctree-l2"><a class="reference internal" href="../container_test.html#duplicate-the-master-container-image">Duplicate the “master” container image</a></li>
<li class="toctree-l2"><a class="reference internal" href="../container_test.html#install-handy-script">Install handy script</a></li>
<li class="toctree-l2"><a class="reference internal" href="../container_test.html#test-topology">Test topology</a></li>
<li class="toctree-l2"><a class="reference internal" href="../container_test.html#end-station-configs">End station configs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../container_test.html#vpp-configs">VPP configs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../container_test.html#ikev2-certificate-setup">IKEv2 certificate setup</a></li>
<li class="toctree-l2"><a class="reference internal" href="../container_test.html#dhcpv6-server-setup">DHCPv6 server setup</a></li>
<li class="toctree-l2"><a class="reference internal" href="../container_test.html#container-host-interoperation">Container / Host Interoperation</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../featuresbyrelease/index.html">Features by Release</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../troubleshooting/index.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../events/index.html">Events</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../reference/index.html">Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../relatedprojects/index.html">Related Projects</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../archive/index.html">Archive</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../about.html">About</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">The Vector Packet Processor</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../index.html">Use Cases</a> &raquo;</li>
        
          <li><a href="index.html">Contiv/VPP</a> &raquo;</li>
        
      <li>Contiv/VPP Network Operation</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../../_sources/usecases/contiv/NETWORKING.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="contiv-vpp-network-operation">
<h1>Contiv/VPP Network Operation<a class="headerlink" href="#contiv-vpp-network-operation" title="Permalink to this headline">¶</a></h1>
<p>This document describes the network operation of the Contiv/VPP k8s network plugin. It
elaborates the operation and config options of the Contiv IPAM, as well as
details on how the VPP gets programmed by Contiv/VPP control plane.</p>
<p>The following picture shows 2-node k8s deployment of Contiv/VPP, with a VXLAN tunnel
established between the nodes to forward inter-node POD traffic. The IPAM options
are depicted on the Node 1, whereas the VPP programming is depicted on the Node 2.</p>
<p><img alt="Contiv/VPP Architecture" src="../../_images/contiv-networking.png" /></p>
<div class="section" id="contiv-vpp-ipam-ip-address-management">
<h2>Contiv/VPP IPAM (IP Address Management)<a class="headerlink" href="#contiv-vpp-ipam-ip-address-management" title="Permalink to this headline">¶</a></h2>
<p>IPAM in Contiv/VPP is based on the concept of <strong>Node ID</strong>. The Node ID is a number
that uniquely identifies a node in the k8s cluster. The first node is assigned
the ID of 1, the second node 2, etc. If a node leaves the cluster, its
ID is released back to the pool and will be re-used by the next node.</p>
<p>The Node ID is used to calculate per-node IP subnets for PODs
and other internal subnets that need to be unique on each node. Apart from the Node ID,
the input for IPAM calculations is a set of config knobs, which can be specified
in the <code class="docutils literal notranslate"><span class="pre">IPAMConfig</span></code> section of the [Contiv/VPP deployment YAML](../../../k8s/contiv-vpp.yaml):</p>
<ul class="simple">
<li><p><strong>PodSubnetCIDR</strong> (default <code class="docutils literal notranslate"><span class="pre">10.1.0.0/16</span></code>): each pod gets an IP address assigned
from this range. The size of this range (default <code class="docutils literal notranslate"><span class="pre">/16</span></code>) dictates upper limit of
POD count for the entire k8s cluster (default 65536 PODs).</p></li>
<li><p><strong>PodNetworkPrefixLen</strong> (default <code class="docutils literal notranslate"><span class="pre">24</span></code>): per-node dedicated podSubnet range.
From the allocatable range defined in <code class="docutils literal notranslate"><span class="pre">PodSubnetCIDR</span></code>, this value will dictate the
allocation for each node. With the default value (<code class="docutils literal notranslate"><span class="pre">24</span></code>) this indicates that each node
has a <code class="docutils literal notranslate"><span class="pre">/24</span></code> slice of the <code class="docutils literal notranslate"><span class="pre">PodSubnetCIDR</span></code>. The Node ID is used to address the node.
In case of <code class="docutils literal notranslate"><span class="pre">PodSubnetCIDR</span> <span class="pre">=</span> <span class="pre">10.1.0.0/16</span></code>, <code class="docutils literal notranslate"><span class="pre">PodNetworkPrefixLen</span> <span class="pre">=</span> <span class="pre">24</span></code> and <code class="docutils literal notranslate"><span class="pre">NodeID</span> <span class="pre">=</span> <span class="pre">5</span></code>,
the resulting POD subnet for the node would be <code class="docutils literal notranslate"><span class="pre">10.1.5.0/24</span></code>.</p></li>
<li><p><strong>PodIfIPCIDR</strong> (default <code class="docutils literal notranslate"><span class="pre">10.2.1.0/24</span></code>): VPP-internal addresses put the VPP interfaces
facing towards the PODs into L3 mode. This IP range will be reused
on each node, thereby it is never externally addressable outside of the node itself.
The only requirement is that this subnet should not collide with any other IPAM subnet.</p></li>
<li><p><strong>VPPHostSubnetCIDR</strong> (default <code class="docutils literal notranslate"><span class="pre">172.30.0.0/16</span></code>): used for addressing
the interconnect of VPP with the Linux network stack, within the same node.
Since this subnet needs to  be unique on each node, the Node ID is used to determine
the actual subnet used on the node with the combination of <code class="docutils literal notranslate"><span class="pre">VPPHostNetworkPrefixLen</span></code>, <code class="docutils literal notranslate"><span class="pre">PodSubnetCIDR</span></code> and <code class="docutils literal notranslate"><span class="pre">PodNetworkPrefixLen</span></code>.</p></li>
<li><p><strong>VPPHostNetworkPrefixLen</strong> (default <code class="docutils literal notranslate"><span class="pre">24</span></code>): used to calculate the subnet
for addressing the interconnect of VPP with the Linux network stack, within the same node.
With <code class="docutils literal notranslate"><span class="pre">VPPHostSubnetCIDR</span> <span class="pre">=</span> <span class="pre">172.30.0.0/16</span></code>, <code class="docutils literal notranslate"><span class="pre">VPPHostNetworkPrefixLen</span> <span class="pre">=</span> <span class="pre">24</span></code> and
<code class="docutils literal notranslate"><span class="pre">NodeID</span> <span class="pre">=</span> <span class="pre">5</span></code> the resulting subnet for the node would be <code class="docutils literal notranslate"><span class="pre">172.30.5.0/24</span></code>.</p></li>
<li><p><strong>NodeInterconnectCIDR</strong> (default <code class="docutils literal notranslate"><span class="pre">192.168.16.0/24</span></code>): range for the addresses
assigned to the data plane interfaces managed by VPP. Unless DHCP is used
(<code class="docutils literal notranslate"><span class="pre">NodeInterconnectDHCP</span> <span class="pre">=</span> <span class="pre">True</span></code>), the Contiv/VPP control plane automatically assigns
an IP address from this range to the DPDK-managed ethernet interface bound to VPP
on each node. The actual IP address will be calculated from the Node ID (e.g., with
<code class="docutils literal notranslate"><span class="pre">NodeInterconnectCIDR</span> <span class="pre">=</span> <span class="pre">192.168.16.0/24</span></code> and <code class="docutils literal notranslate"><span class="pre">NodeID</span> <span class="pre">=</span> <span class="pre">5</span></code>, the resulting IP
address assigned to the ethernet interface on VPP will be <code class="docutils literal notranslate"><span class="pre">192.168.16.5</span></code> ).</p></li>
<li><p><strong>NodeInterconnectDHCP</strong> (default <code class="docutils literal notranslate"><span class="pre">False</span></code>): instead of assigning the IPs
for the data plane interfaces, which are managed by VPP from <code class="docutils literal notranslate"><span class="pre">NodeInterconnectCIDR</span></code> by the Contiv/VPP
control plane, DHCP assigns the IP addresses. The DHCP must be running in the network where the data
plane interface is connected, in case <code class="docutils literal notranslate"><span class="pre">NodeInterconnectDHCP</span> <span class="pre">=</span> <span class="pre">True</span></code>,
<code class="docutils literal notranslate"><span class="pre">NodeInterconnectCIDR</span></code> is ignored.</p></li>
<li><p><strong>VxlanCIDR</strong> (default <code class="docutils literal notranslate"><span class="pre">192.168.30.0/24</span></code>): in order to provide inter-node
POD to POD connectivity via any underlay network (not necessarily an L2 network),
Contiv/VPP sets up a VXLAN tunnel overlay between each of the 2 nodes within the cluster. Each node needs its unique IP address of the VXLAN BVI interface. This IP address
is automatically calculated from the Node ID, (e.g., with <code class="docutils literal notranslate"><span class="pre">VxlanCIDR</span> <span class="pre">=</span> <span class="pre">192.168.30.0/24</span></code>
and <code class="docutils literal notranslate"><span class="pre">NodeID</span> <span class="pre">=</span> <span class="pre">5</span></code>, the resulting IP address assigned to the VXLAN BVI interface will be <code class="docutils literal notranslate"><span class="pre">192.168.30.5</span></code>).</p></li>
</ul>
</div>
<div class="section" id="vpp-programming">
<h2>VPP Programming<a class="headerlink" href="#vpp-programming" title="Permalink to this headline">¶</a></h2>
<p>This section describes how the Contiv/VPP control plane programs VPP,  based on the
events it receives from k8s. This section is not necessarily for understanding
basic Contiv/VPP operation, but is very useful for debugging purposes.</p>
<p>Contiv/VPP currently uses a single VRF to forward the traffic between PODs on a node,
PODs on different nodes, host network stack, and DPDK-managed dataplane interface. The forwarding
between each of them is purely L3-based, even for cases of communication
between 2 PODs within the same node.</p>
<div class="section" id="dpdk-managed-data-interface">
<h3>DPDK-Managed Data Interface<a class="headerlink" href="#dpdk-managed-data-interface" title="Permalink to this headline">¶</a></h3>
<p>In order to allow inter-node communication between PODs on different
nodes and between PODs and outside world, Contiv/VPP uses data-plane interfaces
bound to VPP using DPDK. Each node should have one “main” VPP interface,
which is unbound from the host network stack and bound to VPP.
The Contiv/VPP control plane automatically configures the interface either
via DHCP, or with a statically assigned address (see <code class="docutils literal notranslate"><span class="pre">NodeInterconnectCIDR</span></code> and
<code class="docutils literal notranslate"><span class="pre">NodeInterconnectDHCP</span></code> yaml settings).</p>
</div>
<div class="section" id="pods-on-the-same-node">
<h3>PODs on the Same Node<a class="headerlink" href="#pods-on-the-same-node" title="Permalink to this headline">¶</a></h3>
<p>PODs are connected to VPP using virtio-based TAP interfaces created by VPP,
with the POD-end of the interface placed into the POD container network namespace.
Each POD is assigned an IP address from the <code class="docutils literal notranslate"><span class="pre">PodSubnetCIDR</span></code>. The allocated IP
is configured with the prefix length <code class="docutils literal notranslate"><span class="pre">/32</span></code>. Additionally, a static route pointing
towards the VPP is configured in the POD network namespace.
The  prefix length <code class="docutils literal notranslate"><span class="pre">/32</span></code> means that all IP traffic will be forwarded to the
default route - VPP. To get rid of unnecessary broadcasts between POD and VPP,
a static ARP entry is configured for the gateway IP in the POD namespace, as well
as for POD IP on VPP. Both ends of the TAP interface have a static (non-default)
MAC address applied.</p>
</div>
<div class="section" id="pods-with-hostnetwork-true">
<h3>PODs with hostNetwork=true<a class="headerlink" href="#pods-with-hostnetwork-true" title="Permalink to this headline">¶</a></h3>
<p>PODs with a <code class="docutils literal notranslate"><span class="pre">hostNetwork=true</span></code> attribute are not placed into a separate network namespace, they instead use the main host Linux network namespace; therefore, they are not directly connected to the VPP. They rely on the interconnection between the VPP and the host Linux network stack,
which is described in the next paragraph. Note, when these PODs access some service IP, their network communication will be NATed in Linux (by iptables rules programmed by kube-proxy)
as opposed to VPP, which is the case for the PODs connected to VPP directly.</p>
</div>
<div class="section" id="linux-host-network-stack">
<h3>Linux Host Network Stack<a class="headerlink" href="#linux-host-network-stack" title="Permalink to this headline">¶</a></h3>
<p>In order to interconnect the Linux host network stack with VPP (to allow access
to the cluster resources from the host itself, as well as for the PODs with <code class="docutils literal notranslate"><span class="pre">hostNetwork=true</span></code>),
VPP creates a TAP interface between VPP and the main network namespace. The TAP interface  is configured with IP addresses from the <code class="docutils literal notranslate"><span class="pre">VPPHostSubnetCIDR</span></code> range, with <code class="docutils literal notranslate"><span class="pre">.1</span></code> in the latest octet on the VPP side, and <code class="docutils literal notranslate"><span class="pre">.2</span></code> on the host side. The name of the host interface is <code class="docutils literal notranslate"><span class="pre">vpp1</span></code>. The host has static routes pointing to VPP configured with:</p>
<ul class="simple">
<li><p>A route to the whole <code class="docutils literal notranslate"><span class="pre">PodSubnetCIDR</span></code> to route traffic targeting PODs towards VPP.</p></li>
<li><p>A route to <code class="docutils literal notranslate"><span class="pre">ServiceCIDR</span></code> (default <code class="docutils literal notranslate"><span class="pre">10.96.0.0/12</span></code>), to route service IP targeted traffic that has not been translated by kube-proxy for some reason towards VPP.</p></li>
<li><p>The host also has a static ARP entry configured for the IP of the VPP-end TAP interface, to get rid of unnecessary broadcasts between the main network namespace and VPP.</p></li>
</ul>
</div>
<div class="section" id="vxlans-to-other-nodes">
<h3>VXLANs to Other Nodes<a class="headerlink" href="#vxlans-to-other-nodes" title="Permalink to this headline">¶</a></h3>
<p>In order to provide inter-node POD to POD connectivity via any underlay network
(not necessarily an L2 network), Contiv/VPP sets up a VXLAN tunnel overlay between
each 2 nodes within the cluster (full mesh).</p>
<p>All VXLAN tunnels are terminated in one bridge domain on each VPP. The bridge domain
has learning and flooding disabled, the l2fib of the bridge domain contains a static entry for each VXLAN tunnel. Each bridge domain has a BVI interface, which
interconnects the bridge domain with the main VRF (L3 forwarding). This interface needs
a unique IP address, which is assigned from the <code class="docutils literal notranslate"><span class="pre">VxlanCIDR</span></code> as describe above.</p>
<p>The main VRF contains several static routes that point to the BVI IP addresses of other nodes.
For each node, it is a route to PODSubnet and VppHostSubnet of the remote node, as well as a route
to the management IP address of the remote node. For each of these routes, the next hop IP is the
BVI interface IP of the remote node, which goes via the BVI interface of the local node.</p>
<p>The VXLAN tunnels and the static routes pointing to them are added/deleted on each VPP,
whenever a node is added/deleted in the k8s cluster.</p>
</div>
<div class="section" id="more-info">
<h3>More Info<a class="headerlink" href="#more-info" title="Permalink to this headline">¶</a></h3>
<p>Please refer to the [Packet Flow Dev Guide](../dev-guide/PACKET_FLOW.html) for more
detailed description of paths traversed by request and response packets
inside Contiv/VPP Kubernetes cluster under different situations.</p>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="SINGLE_NIC_SETUP.html" class="btn btn-neutral float-right" title="Setting up a Node with a Single NIC" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="VMWARE_FUSION_HOST.html" class="btn btn-neutral float-left" title="Preparing a VmWare Fusion Host" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018-2020, Linux Foundation

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>